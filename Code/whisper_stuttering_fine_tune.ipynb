{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Whisper for Stuttering Speech\n",
    "\n",
    "This notebook demonstrates how to fine-tune the OpenAI Whisper model on the timestamped FluencyBank dataset, which contains stuttered speech. The process includes:\n",
    "\n",
    "1. Loading and preprocessing the CSV and audio data\n",
    "2. Creating training and testing splits\n",
    "3. Evaluating the base Whisper model's performance on stuttered speech\n",
    "4. Fine-tuning Whisper using transfer learning\n",
    "5. Evaluating the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, we need to install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install openai-whisper\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# !pip install librosa\n",
    "# !pip install jiwer\n",
    "# !pip install accelerate\n",
    "# !pip install soundfile\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x70b985007a30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import whisper\n",
    "import torch\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm  # automatically picks best available tqdm (notebook or CLI)\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset, Audio\n",
    "from jiwer import wer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "First, let's load the CSV files and process them to create a dataset with audio segments and their corresponding transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1128 CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Paths to data\n",
    "csv_dir = os.path.expanduser(\"~/TimeStamped/cleaned_csvs\")\n",
    "audio_dir = os.path.expanduser(\"~/TimeStamped-wav/combined_wavs\")\n",
    "\n",
    "# Get all CSV files\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments_from_csv(csv_file):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    segments = []\n",
    "\n",
    "    # Extract base filename (without extension), e.g. \"24fa_000_combined\"\n",
    "    base_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "    # Build the corresponding WAV file path\n",
    "    wav_file = os.path.join(audio_dir, f\"{base_name}.wav\")\n",
    "\n",
    "    if not os.path.exists(wav_file):\n",
    "        print(f\"Warning: Audio file {wav_file} not found. Skipping file {csv_file}.\")\n",
    "        return segments\n",
    "\n",
    "    # Use the base_name as segid\n",
    "    segid = base_name\n",
    "\n",
    "    start_time = df['wordstart'].min()\n",
    "    end_time = df['wordend'].max()\n",
    "\n",
    "    words = df.sort_values('wordstart')['word'].tolist()\n",
    "    transcript = ' '.join(words)\n",
    "\n",
    "    has_stutter = any(df['fp'] == 1) or any(df['rp'] == 1) or any(df['rv'] == 1) or any(df['pw'] == 1)\n",
    "\n",
    "    segments.append({\n",
    "        'segid': segid,\n",
    "        'wav_file': wav_file,\n",
    "        'start_time': start_time,\n",
    "        'end_time': end_time,\n",
    "        'transcript': transcript,\n",
    "        'has_stutter': has_stutter\n",
    "    })\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cef15f3498941028ea012675f1d9798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSV files:   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments extracted: 1128\n",
      "\n",
      "Sample segments:\n",
      "Segment ID: 44m_216_combined\n",
      "WAV file: /home/ubuntu/TimeStamped-wav/combined_wavs/44m_216_combined.wav\n",
      "Time range: 0.05 - 8.39 seconds\n",
      "Transcript: it's not just dependent on me it's dependent on um the speaker and the listener and if you're um i guess both parties have do an act of communication\n",
      "Has stutter: True\n",
      "---\n",
      "Segment ID: 24mb_108_combined\n",
      "WAV file: /home/ubuntu/TimeStamped-wav/combined_wavs/24mb_108_combined.wav\n",
      "Time range: 0.05 - 5.58 seconds\n",
      "Transcript: and if i need to repeat myself then i'll repeat myself but as long as the person understands what i'm saying\n",
      "Has stutter: False\n",
      "---\n",
      "Segment ID: 24fb_015_combined\n",
      "WAV file: /home/ubuntu/TimeStamped-wav/combined_wavs/24fb_015_combined.wav\n",
      "Time range: 0.05 - 27.11 seconds\n",
      "Transcript: and in at the end i didn't go because i learned anything because i went for the social part it was nice the being the being there and yeah um well uh that's a difficult question um positive communication i don't um\n",
      "Has stutter: True\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Extract segments from all CSV files\n",
    "all_segments = []\n",
    "for csv_file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    segments = extract_segments_from_csv(csv_file)\n",
    "    all_segments.extend(segments)\n",
    "\n",
    "print(f\"Total segments extracted: {len(all_segments)}\")\n",
    "\n",
    "# Display sample segments\n",
    "print(\"\\nSample segments:\")\n",
    "for segment in all_segments[:3]:\n",
    "    print(f\"Segment ID: {segment['segid']}\")\n",
    "    print(f\"WAV file: {segment['wav_file']}\")\n",
    "    print(f\"Time range: {segment['start_time']} - {segment['end_time']} seconds\")\n",
    "    print(f\"Transcript: {segment['transcript']}\")\n",
    "    print(f\"Has stutter: {segment['has_stutter']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train and Test Splits\n",
    "\n",
    "Next, let's split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 790\n",
      "Validation segments: 169\n",
      "Test segments: 169\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the segments\n",
    "random.shuffle(all_segments)\n",
    "\n",
    "# Split into train, validation, and test sets (70% train, 15% validation, 15% test)\n",
    "total_segments = len(all_segments)\n",
    "val_count = max(1, int(total_segments * 0.15))\n",
    "test_count = max(1, int(total_segments * 0.15))\n",
    "train_count = total_segments - val_count - test_count\n",
    "\n",
    "# Create the splits\n",
    "test_segments = all_segments[:test_count]\n",
    "val_segments = all_segments[test_count:test_count + val_count]\n",
    "train_segments = all_segments[test_count + val_count:]\n",
    "\n",
    "print(f\"Train segments: {len(train_segments)}\")\n",
    "print(f\"Validation segments: {len(val_segments)}\")\n",
    "print(f\"Test segments: {len(test_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split segments into train and test sets based on speaker ID\n",
    "# train_segments = []\n",
    "# test_segments = []\n",
    "\n",
    "# for segment in processed_segments:\n",
    "#     speaker_id = segment['segid'].split('_')[0]\n",
    "#     if speaker_id in test_speakers:\n",
    "#         test_segments.append(segment)\n",
    "#     else:\n",
    "#         train_segments.append(segment)\n",
    "\n",
    "# print(f\"Train segments: {len(train_segments)}\")\n",
    "# print(f\"Test segments: {len(test_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split information for later reference\n",
    "split_info = {\n",
    "    'train_segment_ids': [seg['segid'] for seg in train_segments],\n",
    "    'val_segment_ids': [seg['segid'] for seg in val_segments],\n",
    "    'test_segment_ids': [seg['segid'] for seg in test_segments],\n",
    "    'train_count': len(train_segments),\n",
    "    'val_count': len(val_segments),\n",
    "    'test_count': len(test_segments)\n",
    "}\n",
    "\n",
    "with open(f'data_split_info-{TRAIN_NUM}.json', 'w') as f:\n",
    "    json.dump(split_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets for HuggingFace\n",
    "\n",
    "Now, let's prepare the data for training with the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(segments):\n",
    "    data = {\n",
    "        'audio': [],\n",
    "        'text': []\n",
    "    }\n",
    "    \n",
    "    for segment in segments:\n",
    "        data['audio'].append(segment['wav_file'])\n",
    "        data['text'].append(segment['transcript'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 790\n",
      "})\n",
      "Validation dataset: Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 169\n",
      "})\n",
      "Test dataset: Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 169\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Prepare the train, validation, and test datasets\n",
    "train_data = prepare_dataset(train_segments)\n",
    "val_data = prepare_dataset(val_segments)\n",
    "test_data = prepare_dataset(test_segments)\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Add audio loading capability\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Validation dataset: {val_dataset}\")\n",
    "print(f\"Test dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Base Whisper Model\n",
    "\n",
    "Let's first evaluate the base Whisper model on our test set to establish a baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the base Whisper model\n",
    "def evaluate_base_whisper(test_dataset, model_size=\"base\"):\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(model_size)\n",
    "    print(f\"Loaded Whisper {model_size} model.\")\n",
    "    \n",
    "    # Extract audio paths and transcripts from the dataset\n",
    "    audio_paths = test_dataset['audio']\n",
    "    references = test_dataset['text']\n",
    "    \n",
    "    hypotheses = []\n",
    "    \n",
    "    # Process each audio file\n",
    "    for idx, audio_path in enumerate(tqdm(audio_paths, desc=f\"Evaluating {model_size} Whisper\")):\n",
    "        try:\n",
    "            # Get the actual file path from the dataset info\n",
    "            audio_file = audio_path['path'] if isinstance(audio_path, dict) else audio_path\n",
    "            \n",
    "            # Transcribe the audio with explicit settings\n",
    "            result = model.transcribe(\n",
    "                audio_file,\n",
    "                language='en',  # Force English language\n",
    "                task='transcribe'  # Ensure transcription instead of translation\n",
    "            )\n",
    "            \n",
    "            # Get the transcription\n",
    "            transcription = result[\"text\"].strip()\n",
    "            hypotheses.append(transcription)\n",
    "            \n",
    "            # Print progress every 10 files\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(audio_paths)} files\")\n",
    "                print(f\"Reference: {references[idx]}\")\n",
    "                print(f\"Hypothesis: {transcription}\")\n",
    "                print(\"---\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {audio_file}: {e}\")\n",
    "            hypotheses.append(\"\")\n",
    "    \n",
    "    # Calculate WER\n",
    "    error_rate = wer(references, hypotheses)\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'references': references,\n",
    "        'hypotheses': hypotheses,\n",
    "        'wer': error_rate\n",
    "    }\n",
    "    \n",
    "    with open(f'base_whisper_{model_size}_results.json-{TRAIN_NUM}', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return error_rate, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Whisper small model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc384c3ca32246f4b15b47804f0ec81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating small Whisper:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/169 files\n",
      "Reference: i would cheat i would write down those instances on a piece of paper just turn it in but yes they taught me some of the basic techniques of easy onsets\n",
      "Hypothesis: I would write down those instances on a piece of paper and just turn it in. But yes, they taught me some of the basic techniques of easy onsets.\n",
      "---\n",
      "Processed 20/169 files\n",
      "Reference: so it's uh uh there's some mind games there i think that happen um i think sometimes i still get um more hesitant to kind of put myself out there in certain social situations with people uh who i don't know but i'm you know i'm a fairly like an outgoing person so i try not to let it get into my way but i can see that there you know are certain times where it's still uh probably does\n",
      "Hypothesis: So it's, it's, it's, there's some mind games there. I think that happened. I think sometimes I still get more hesitant to kind of put myself out there in certain social situations with people who I don't know, but I'm, you know, I'm a fairly, like an outgoing person. So I try not to let it get into my way, but I can see that there are certain times where it still probably, probably does.\n",
      "---\n",
      "Processed 30/169 files\n",
      "Reference: and how differently stuttering was viewed uh dramatically differently from the united states and uh we laughed a lot and i learned a lot and it was a lot of fun\n",
      "Hypothesis: and how differently stuttering with viewed, dramatically differently from the United States. And we laughed a lot and I learned a lot and it was a lot of fun.\n",
      "---\n",
      "Processed 40/169 files\n",
      "Reference: participation in community activities that simply didn't happen that would been voluntary and uh that simply didn't happen\n",
      "Hypothesis: Participation in community activities that since something didn't happen, that would have been voluntary and that simply didn't happen.\n",
      "---\n",
      "Processed 50/169 files\n",
      "Reference: so some days i speak fairly fluently without much effort and other days i struggle more naturally and it's just the way that it is it's not that i'm more anxious one day or another day\n",
      "Hypothesis: So some days I speak fairly fluently without much effort and other days I struggle more naturally and it's just the way that it is. It's not that I'm more anxious one day or another day.\n",
      "---\n",
      "Processed 60/169 files\n",
      "Reference: sure um well so in my uh daily life i'm uh in the army so i'm uh frequently um speaking to uh crowds uh of soldiers or uh other officers uh and so uh sometimes i have to start by being very directive to uh gain control uh of the uh situation\n",
      "Hypothesis: Sure, well, so in my daily life I'm in the army so I'm frequently speaking to crowds of soldiers or other officers. And so sometimes I have to start by being very directive to gain control of the situation.\n",
      "---\n",
      "Processed 70/169 files\n",
      "Reference: sure well um well you know well um my father was a stutterer until he was fourteen and then he you know uh you know stopped um my parents pretty much thought i would just spontaneously quit stuttering so\n",
      "Hypothesis: Sure. Well, um, well, you know, well, um, my father was a stutterer until he was 14. And then he stopped, you know, stopped, you know, stopped. Um, my parents pretty much thought I would just spontaneously quit stuttering. So.\n",
      "---\n",
      "Processed 80/169 files\n",
      "Reference: and he you know sometimes um that's been a detriment but i think on the whole um you know it's probably made me a better person hmm i don't know um you know i would say uh uh learn about the\n",
      "Hypothesis: And you know sometimes that's been a detriment but I think on the whole it's you know it's probably made me a better person. I don't know. You know I would say I would say learn about the end.\n",
      "---\n",
      "Processed 90/169 files\n",
      "Reference: and then doing things like this uh for nsa and one of the things that i'm doing\n",
      "Hypothesis: And then doing things like this for NSA and one of the things that I'm doing.\n",
      "---\n",
      "Processed 100/169 files\n",
      "Reference: if you didn't stutter what might be different in your life i can give um uh one word answer to that that would be everything absolutely everything would be different in my life\n",
      "Hypothesis: If you didn't stutter, what might be different in your life? I can give one word answer to that. That would be everything, absolutely everything would be different in my life.\n",
      "---\n",
      "Processed 110/169 files\n",
      "Reference: um i think lots of i guess the cause the actual physical mechanism that creates a disfluency it seems that most of the research is suggesting it's has uh genetic origins or there you have a genetic predisposition to stutter\n",
      "Hypothesis: I think lots of, I guess, that cause the actual physical mechanism that creates a disfluency. It seems that most of the researchers suggesting it has genetic origins or you have a genetic predisposition to stutter.\n",
      "---\n",
      "Processed 120/169 files\n",
      "Reference: um it's a there's depending on the depth of it perhaps there's even a level of uh trust established um there's a relationship building\n",
      "Hypothesis: It's a, there's depending on the depth of it, perhaps there's even a level of trust established. There's a, there's a relationship building.\n",
      "---\n",
      "Processed 130/169 files\n",
      "Reference: i know that you can have a fluent person that has trauma and then they now are a stutterer always and that's you know i don't know\n",
      "Hypothesis: I know that you can have a fluent person that has trauma and then they now are a stutterer always and that's, you know, I don't know.\n",
      "---\n",
      "Processed 140/169 files\n",
      "Reference: i don't know but i switched from that to respiratory care so yeah\n",
      "Hypothesis: I don't know, but I switched from that to respiratory care. So that would definitely have been an impact.\n",
      "---\n",
      "Processed 150/169 files\n",
      "Reference: um uh uh if hobbies are a part of community activities uh um from eighteen to thirty two i was uh competing in uh amateur automobile racing uh the um with the sports car club of america scca um uh my in my late twenties i became four time regional champion of autocross and rallycross in my region\n",
      "Hypothesis: If hobbies are part of community activities, from 18 to 32 I was competing in amateur automobile racing with the Sports Car Club of America, SCCA. In my late 20's I became 4-time regional champion of Autopross and Ridecross in my region.\n",
      "---\n",
      "Processed 160/169 files\n",
      "Reference: so as a therapist as a social worker as a supervisor as presenter as a speaker however you wanna look at it people tend to open up to me a quite a lot faster\n",
      "Hypothesis: So as a therapist, as a social worker, as a supervisor, as a presenter, as a speaker, however you want to look at it, people tend to open up to me quite a lot faster.\n",
      "---\n",
      "small Whisper WER: 0.3674\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the base Whisper model\n",
    "model_size=\"small\" #changing to small to see better results\n",
    "# model_size=\"base\" #FIXME\n",
    "base_wer, base_results = evaluate_base_whisper(test_dataset, model_size)\n",
    "print(f\"{model_size} Whisper WER: {base_wer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Whisper\n",
    "\n",
    "Now, let's fine-tune the Whisper model on our stuttering speech dataset using the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Whisper processor and model for fine-tuning\n",
    "# model_id = \"openai/whisper-base\"\n",
    "model_id = \"openai/whisper-small\" #FIXME - change model based on ability\n",
    "# model_id = \"openai/whisper-medium\"\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the dataset for fine-tuning\n",
    "def prepare_dataset_for_finetuning(dataset, processor):\n",
    "    # Define a preprocessing function\n",
    "    def prepare_example(example):\n",
    "        # Load and resample the audio data\n",
    "        audio = example[\"audio\"]\n",
    "        \n",
    "        # Process the audio input\n",
    "        input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features[0]\n",
    "        \n",
    "        # Process the text output\n",
    "        example[\"labels\"] = processor(text=example[\"text\"]).input_ids\n",
    "        example[\"input_features\"] = input_features\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    # Process the dataset\n",
    "    processed_dataset = dataset.map(prepare_example, remove_columns=[\"audio\", \"text\"])\n",
    "    \n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0439ded25fd04a3fac9e771388aa930e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/790 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d929ad145b142c7a61e3ef79f81f595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a650573df544adb11f2c593b7253bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset: Dataset({\n",
      "    features: ['labels', 'input_features'],\n",
      "    num_rows: 790\n",
      "})\n",
      "Processed validation dataset: Dataset({\n",
      "    features: ['labels', 'input_features'],\n",
      "    num_rows: 169\n",
      "})\n",
      "Processed test dataset: Dataset({\n",
      "    features: ['labels', 'input_features'],\n",
      "    num_rows: 169\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Prepare the datasets for fine-tuning\n",
    "print(\"Preparing train dataset...\")\n",
    "processed_train_dataset = prepare_dataset_for_finetuning(train_dataset, processor)\n",
    "\n",
    "print(\"Preparing validation dataset...\")\n",
    "processed_val_dataset = prepare_dataset_for_finetuning(val_dataset, processor)\n",
    "\n",
    "print(\"Preparing test dataset...\")\n",
    "processed_test_dataset = prepare_dataset_for_finetuning(test_dataset, processor)\n",
    "\n",
    "print(f\"Processed train dataset: {processed_train_dataset}\")\n",
    "print(f\"Processed validation dataset: {processed_val_dataset}\")\n",
    "print(f\"Processed test dataset: {processed_test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Iterations - Including for Insight into Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-fine-tuned-stuttering\",\n",
    "#     per_device_train_batch_size=4,  # Reduced further from 8 to 4\n",
    "#     gradient_accumulation_steps=4,  # Increased from 2 to 4 to maintain effective batch size\n",
    "#     learning_rate=1e-5,\n",
    "#     warmup_steps=125,\n",
    "#     max_steps=1000,\n",
    "#     gradient_checkpointing=True,  # Already enabled - saves memory\n",
    "#     fp16=True,  # Already enabled - uses half precision\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=125,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=125,\n",
    "#     logging_steps=50,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"wer\",\n",
    "#     greater_is_better=False,\n",
    "#     push_to_hub=False,\n",
    "#     dataloader_num_workers=0,  # Avoid multiprocessing overhead\n",
    "#     optim=\"adafactor\",  # More memory-efficient optimizer\n",
    "#     eval_accumulation_steps=8,  # Accumulate gradients during evaluation\n",
    "#     prediction_loss_only=False,\n",
    "#     group_by_length=True,  # Efficient batching by length\n",
    "#     label_smoothing_factor=0.1,  # Regularization that can help training\n",
    "# )\n",
    "\n",
    "# # Additional memory optimizations\n",
    "# model.config.use_cache = False  # Disable KV cache\n",
    "# torch.cuda.empty_cache()  # Clear CUDA cache before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-fine-tuned-stuttering-test\",\n",
    "#     per_device_train_batch_size=1,          # small batch for quick test\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     learning_rate=1e-4,                     # slightly higher LR for quick learning\n",
    "#     warmup_steps=5,\n",
    "#     max_steps=100,                           # ðŸ”¥ just 10 steps\n",
    "#     eval_strategy=\"no\",                     # skip evaluation for test run\n",
    "#     save_strategy=\"no\",                     # no saving checkpoints\n",
    "#     logging_steps=1,\n",
    "#     gradient_checkpointing=False,\n",
    "#     fp16=torch.cuda.is_available(),         # only use fp16 if CUDA is available\n",
    "#     report_to=[],                           # no TensorBoard in test\n",
    "#     push_to_hub=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-fine-tuned-stuttering-test\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     learning_rate=1e-4,\n",
    "#     warmup_steps=100,\n",
    "#     max_steps=500,  \n",
    "#     eval_strategy=\"no\",\n",
    "#     logging_steps=10,\n",
    "#     gradient_checkpointing=False,\n",
    "#     save_strategy=\"no\",\n",
    "#     report_to=[],\n",
    "#     push_to_hub=False,\n",
    "#     fp16=torch.cuda.is_available()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training arguments optimized for heavy training without evaluation **DOING GREAT!\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-fine-tuned-stuttering2\",\n",
    "#     per_device_train_batch_size=4,          # Larger batch size for faster training\n",
    "#     gradient_accumulation_steps=2,          # Effective batch size of 8\n",
    "#     learning_rate=5e-5,\n",
    "#     warmup_steps=200,\n",
    "#     max_steps=1000,                         # More steps for thorough training\n",
    "#     eval_strategy=\"no\",                     # Disable evaluation completely\n",
    "#     # save_strategy=\"steps\",                  \n",
    "#     # save_steps=1000,                        # Save checkpoints less frequently\n",
    "#     logging_steps=20,                       # Regular logging for monitoring\n",
    "#     # gradient_checkpointing=True,            # Enable for memory efficiency\n",
    "#     fp16=torch.cuda.is_available(),                             # Use mixed precision\n",
    "#     # dataloader_num_workers=4,              # Use multiple workers for data loading\n",
    "#     optim=\"adamw_torch\",                   \n",
    "#     push_to_hub=False,\n",
    "#     # save_total_limit=3,                    # Keep only 3 most recent checkpoints\n",
    "#     report_to=[\"tensorboard\"],             # Still log to tensorboard for monitoring\n",
    "#     prediction_loss_only=True,             # Only compute loss, not other metrics\n",
    "#     # group_by_length=True,                  # Efficient batching\n",
    "#     # label_smoothing_factor=0.1,            # Regularization\n",
    "# )\n",
    "\n",
    "# # Memory optimizations\n",
    "# model.config.use_cache = False\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE TO CHANGE TRAIN_NUM\n",
    "this is main training block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for heavy training without evaluation BEST MODEL\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./whisper-fine-tuned-stuttering-{TRAIN_NUM}\",\n",
    "    # per_device_train_batch_size=8,          # changed this\n",
    "    per_device_train_batch_size=4,          # changed to 4 for medium model!\n",
    "    gradient_accumulation_steps=4,          # Changed this\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,                       # Changed this\n",
    "    max_steps=2000,                         # More steps for thorough training\n",
    "    eval_strategy=\"no\",                     # Disable evaluation completely\n",
    "    # save_strategy=\"steps\",                  \n",
    "    # save_steps=1000,                        \n",
    "    logging_steps=40,                       # changed\n",
    "    # gradient_checkpointing=True,          \n",
    "    fp16=torch.cuda.is_available(),                            \n",
    "    # dataloader_num_workers=4,         \n",
    "    optim=\"adamw_torch\",                   \n",
    "    push_to_hub=False,\n",
    "    # save_total_limit=3,                    \n",
    "    report_to=[\"tensorboard\"],        \n",
    "    prediction_loss_only=True,          \n",
    "    # group_by_length=True,             \n",
    "    # label_smoothing_factor=0.1,        \n",
    ")\n",
    "\n",
    "# Memory optimizations\n",
    "model.config.use_cache = False\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enhanced training arguments for potentially better results with ~700 clips dataset\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-fine-tuned-stuttering-final-{TRAIN_NUM}\",\n",
    "#     # Batch size and accumulation\n",
    "#     per_device_train_batch_size=4,           # Good balance for available memory\n",
    "#     gradient_accumulation_steps=4,           # Effective batch size of 16\n",
    "    \n",
    "#     # Learning rate schedule\n",
    "#     learning_rate=2e-5,                      # Slightly lower learning rate for stability\n",
    "#     lr_scheduler_type=\"cosine\",              # Cosine schedule works well for speech models\n",
    "#     warmup_ratio=0.1,                        # Warmup over 10% of training\n",
    "    \n",
    "#     # Training length\n",
    "#     max_steps=3000,                          # More steps for a dataset of this size\n",
    "    \n",
    "#     # Regularization\n",
    "#     weight_decay=0.01,                       # Help prevent overfitting (for small dataset)\n",
    "    \n",
    "#     # No evaluation to save memory\n",
    "#     eval_strategy=\"no\",\n",
    "    \n",
    "#     # Memory settings\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     fp16_full_eval=False,\n",
    "#     gradient_checkpointing=True,             # Enable for memory efficiency\n",
    "    \n",
    "#     # Logging\n",
    "#     logging_steps=50,                        # Log progress more frequently\n",
    "#     # save_strategy=\"steps\",                   # Save at regular intervals\n",
    "#     # save_steps=500,                          # Save every 500 steps\n",
    "#     # save_total_limit=3,                      # Keep only 3 most recent checkpoints\n",
    "    \n",
    "#     # Better mixing of samples\n",
    "#     # group_by_length=True,                    # Group similar length audios together\n",
    "#     # length_column_name=\"input_features\",     # Group based on input features\n",
    "    \n",
    "#     # Reporting\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     prediction_loss_only=True,\n",
    "#     push_to_hub=False,\n",
    "    \n",
    "#     # Optimize dataloader\n",
    "#     # dataloader_num_workers=0,                # Avoid multiprocessing issues\n",
    "#     # dataloader_pin_memory=True,              # Faster data transfer to GPU\n",
    "# )\n",
    "\n",
    "# # Memory optimizations\n",
    "# model.config.use_cache = False\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Split inputs and labels since they need to be treated differently\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Convert input features to tensors\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Convert labels to tensors with appropriate padding\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # If bos_token_id exists, remove it\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# Create the data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute metrics function\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # Replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # Convert ids to strings\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute WER\n",
    "    error = wer(label_str, pred_str)\n",
    "    \n",
    "    return {\"wer\": error}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with the callback\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with progress monitoring\n",
    "print(\"Starting training with progress monitoring...\")\n",
    "print(f\"Number of training samples: {len(processed_train_dataset)}\")  # ADDED\n",
    "print(f\"Number of validation samples: {len(processed_val_dataset)}\")  # ADDED\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(f\"./whisper-fine-tuned-stuttering-final-{TRAIN_NUM}\")\n",
    "processor.save_pretrained(f\"./whisper-fine-tuned-stuttering-final-{TRAIN_NUM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model\n",
    "\n",
    "Now, let's evaluate the fine-tuned model on our test set to see how it performs compared to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_processor = WhisperProcessor.from_pretrained(f\"./whisper-fine-tuned-stuttering-final-{TRAIN_NUM}\")\n",
    "fine_tuned_model = WhisperForConditionalGeneration.from_pretrained(f\"./whisper-fine-tuned-stuttering-final-{TRAIN_NUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the fine-tuned model\n",
    "def evaluate_fine_tuned_whisper(test_dataset, processor, model, test=1):\n",
    "    # Extract audio paths and transcripts from the dataset\n",
    "    audio_paths = test_dataset['audio']\n",
    "    references = test_dataset['text']\n",
    "    \n",
    "    hypotheses = []\n",
    "    \n",
    "    # Process each audio file\n",
    "    for idx, audio_path in enumerate(tqdm(audio_paths, desc=\"Evaluating fine-tuned Whisper\")):\n",
    "        try:\n",
    "            # Get the actual file path from the dataset info\n",
    "            audio_file = audio_path['path'] if isinstance(audio_path, dict) else audio_path\n",
    "            \n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(audio_file, sr=16000)\n",
    "            \n",
    "            # Process audio for input\n",
    "            input_features = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
    "            \n",
    "            # Clear the forced_decoder_ids and set language and task\n",
    "            model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "            # Generate transcription with explicit arguments\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                language='en',\n",
    "                task='transcribe',\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            hypotheses.append(transcription)\n",
    "            \n",
    "            # Print progress every 10 files\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(audio_paths)} files\")\n",
    "                print(f\"Reference: {references[idx]}\")\n",
    "                print(f\"Hypothesis: {transcription}\")\n",
    "                print(\"---\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {audio_file}: {e}\")\n",
    "            hypotheses.append(\"\")\n",
    "    \n",
    "    # Calculate WER\n",
    "    error_rate = wer(references, hypotheses)\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'references': references,\n",
    "        'hypotheses': hypotheses,\n",
    "        'wer': error_rate\n",
    "    }\n",
    "    \n",
    "    if test==1:\n",
    "        with open(f'fine_tuned_whisper_results_test-{TRAIN_NUM}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "    elif test==0:\n",
    "        with open(f'fine_tuned_whisper_results_val-{TRAIN_NUM}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)       \n",
    "    \n",
    "    return error_rate, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ba9cf9298409493151a2009fa5f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating fine-tuned Whisper:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/169 files\n",
      "Reference: i would cheat i would write down those instances on a piece of paper just turn it in but yes they taught me some of the basic techniques of easy onsets\n",
      "Hypothesis: i would cheat i would write down those instances on the piece of paper and just turn it in but yes they taught me some of the basic techniques of easy onsets\n",
      "---\n",
      "Processed 20/169 files\n",
      "Reference: so it's uh uh there's some mind games there i think that happen um i think sometimes i still get um more hesitant to kind of put myself out there in certain social situations with people uh who i don't know but i'm you know i'm a fairly like an outgoing person so i try not to let it get into my way but i can see that there you know are certain times where it's still uh probably does\n",
      "Hypothesis: so it's uh uh there's some mind games there i think that happen um i think sometimes i still get uh more hesitant to kind of put myself out there in certain social situations with people uh who i don't know but i'm you know i'm a fairly like an outgoing person so i try not to let it get into my way but i can see that there you know are certain times where it's don't probably be a bit technical but it can be a bit technical\n",
      "---\n",
      "Processed 30/169 files\n",
      "Reference: and how differently stuttering was viewed uh dramatically differently from the united states and uh we laughed a lot and i learned a lot and it was a lot of fun\n",
      "Hypothesis: and how differently stuttering would view uh dramatically differently from the united states and uh we laughed a lot and i learned a lot and it was a lot of fun\n",
      "---\n",
      "Processed 40/169 files\n",
      "Reference: participation in community activities that simply didn't happen that would been voluntary and uh that simply didn't happen\n",
      "Hypothesis: participation in community activities that since something didn't happen that would've been voluntary and uh that simply didn't happen\n",
      "---\n",
      "Processed 50/169 files\n",
      "Reference: so some days i speak fairly fluently without much effort and other days i struggle more naturally and it's just the way that it is it's not that i'm more anxious one day or another day\n",
      "Hypothesis: so some days i speak fairly fluently without much effort and other days i struggle more naturally and it's just the way that it is it's not that i'm more anxious one day or another day\n",
      "---\n",
      "Processed 60/169 files\n",
      "Reference: sure um well so in my uh daily life i'm uh in the army so i'm uh frequently um speaking to uh crowds uh of soldiers or uh other officers uh and so uh sometimes i have to start by being very directive to uh gain control uh of the uh situation\n",
      "Hypothesis: sure um well so in my uh daily life i'm uh in the army so i'm uh frequently um speaking to uh crowds of uh uh soldiers or uh other officers uh and um so uh sometimes i have to start by being very directive to uh gain control uh so that i don't run low on air or i'm not really sure what causes stuttering\n",
      "---\n",
      "Processed 70/169 files\n",
      "Reference: sure well um well you know well um my father was a stutterer until he was fourteen and then he you know uh you know stopped um my parents pretty much thought i would just spontaneously quit stuttering so\n",
      "Hypothesis: sure well um well you know well um my father was a stutterer until he was forty and then he stopped you know stopped uh you know stopped um my parents pretty much thought i would just spontaneously quit stuttering so\n",
      "---\n",
      "Processed 80/169 files\n",
      "Reference: and he you know sometimes um that's been a detriment but i think on the whole um you know it's probably made me a better person hmm i don't know um you know i would say uh uh learn about the\n",
      "Hypothesis: and you know sometimes that's been a detriment but i think on the whole um you know it's probably made me a better person hmm i don't know um you know i would say uh uh learned about the\n",
      "---\n",
      "Processed 90/169 files\n",
      "Reference: and then doing things like this uh for nsa and one of the things that i'm doing\n",
      "Hypothesis: and then doing things like this uh for nsa and one of the things that i'm doing\n",
      "---\n",
      "Processed 100/169 files\n",
      "Reference: if you didn't stutter what might be different in your life i can give um uh one word answer to that that would be everything absolutely everything would be different in my life\n",
      "Hypothesis: if you didn't stutter what might be different in your life i can give um a one word answer to that that would be everything absolutely everything would be different in my life\n",
      "---\n",
      "Processed 110/169 files\n",
      "Reference: um i think lots of i guess the cause the actual physical mechanism that creates a disfluency it seems that most of the research is suggesting it's has uh genetic origins or there you have a genetic predisposition to stutter\n",
      "Hypothesis: um i think lots of i guess that cause the actual physical mechanism that creates a disfluency it seems that most of the research is suggesting it's has uh genetic origins or you have a genetic predisposition to stutter\n",
      "---\n",
      "Processed 120/169 files\n",
      "Reference: um it's a there's depending on the depth of it perhaps there's even a level of uh trust established um there's a relationship building\n",
      "Hypothesis: um it's a there's depending on the depth of it perhaps there's even a level of uh trust established um there's a relationship building\n",
      "---\n",
      "Processed 130/169 files\n",
      "Reference: i know that you can have a fluent person that has trauma and then they now are a stutterer always and that's you know i don't know\n",
      "Hypothesis: i know that you can have a fluent person that has trauma and then they now i stutter always and that's you know i don't know\n",
      "---\n",
      "Processed 140/169 files\n",
      "Reference: i don't know but i switched from that to respiratory care so yeah\n",
      "Hypothesis: i don't know but i switch from that to respiratory care so that would definitely have been an impact that\n",
      "---\n",
      "Processed 150/169 files\n",
      "Reference: um uh uh if hobbies are a part of community activities uh um from eighteen to thirty two i was uh competing in uh amateur automobile racing uh the um with the sports car club of america scca um uh my in my late twenties i became four time regional champion of autocross and rallycross in my region\n",
      "Hypothesis: um uh if hollabies are part of community activities uh um from eighteen to thirty two i was uh competing in amateur uh amateur all the racing uh the uh with the sports club of america scca uh my in my like twenty's i was also a part of that race uh\n",
      "---\n",
      "Processed 160/169 files\n",
      "Reference: so as a therapist as a social worker as a supervisor as presenter as a speaker however you wanna look at it people tend to open up to me a quite a lot faster\n",
      "Hypothesis: so i was a therapy person as a social worker as a supervisor as a presenter as a speaker however you want to look at it people tend to open up to me a quite a lot faster\n",
      "---\n",
      "Fine-tuned Whisper WER: 0.1897\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "fine_tuned_wer, fine_tuned_results = evaluate_fine_tuned_whisper(test_dataset, fine_tuned_processor, fine_tuned_model, test=1)\n",
    "print(f\"Fine-tuned Whisper WER: {fine_tuned_wer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Let's compare the performance of the base and fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Whisper WER: 0.3674\n",
      "Fine-tuned Whisper WER: 0.1897\n",
      "Improvement: 48.36%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base Whisper WER: {base_wer:.4f}\")\n",
    "print(f\"Fine-tuned Whisper WER: {fine_tuned_wer:.4f}\")\n",
    "print(f\"Improvement: {(base_wer - fine_tuned_wer) / base_wer * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analysis: Validation Set Performance\n",
    " \n",
    "Let's also check how the model performed on the validation set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c674d7310074721bb58ea2ba8265a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating fine-tuned Whisper:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/169 files\n",
      "Reference: like i would intentionally tell the teacher and luckily they're they don't force me until i was about in high school they respect that\n",
      "Hypothesis: like i have actually something to secure and likely there they didn't watch me so that was quite nice um respect that\n",
      "---\n",
      "Processed 20/169 files\n",
      "Reference: and i commend you for having the compassion to be able to help people because you the impact that a speech language pathologist can make on a stutterer is huge but also on the parents of a stutterer and i think that's something else to where uh the parents need to also have some people to talk to\n",
      "Hypothesis: and i can mend you for having the compassion to be able to help people because you the impact that a speech therapist can make on the stutterer is huge but also on the parents of a stutterer and i think that's something else where uh the parents need to also have some people to talk to\n",
      "---\n",
      "Processed 30/169 files\n",
      "Reference: or gone to a speech therapy and it was um i thought it was very ineffective you know the time i didn't know what to expect\n",
      "Hypothesis: going to speech therapy and it was um i thought it was very effective the time i didn't know what to expect\n",
      "---\n",
      "Processed 40/169 files\n",
      "Reference: you know uh uh to uh become an uh emt you know um um um the instructors um they said that my speech could have been a you know you know problem um in hindsight it might have been a problem or not\n",
      "Hypothesis: to become an eon um um and tio um tio um the instructors um the supervisors um the supervisors uh who aren't worried about my stuttering\n",
      "---\n",
      "Processed 50/169 files\n",
      "Reference: sometimes i wonder if this is actually something that occurred because of the stuttering or is it because i held myself back my answer now is that it's because i held myself back i don't think this had to happen\n",
      "Hypothesis: sometimes i wonder if this is actually something that occurs because stuttering is because i help myself back my answer now is that it's because i help myself back i don't think this had to happen\n",
      "---\n",
      "Processed 60/169 files\n",
      "Reference: it's an educational process about stuttering what do i think causes stuttering well i know a little bit more than just the average person that\n",
      "Hypothesis: it's an educational process about stuttering what do i think causes stuttering well i know a little more than this average person that\n",
      "---\n",
      "Processed 70/169 files\n",
      "Reference: like i wouldn't know what a non stuttering me would even do or you know you know be um might i have a different job possibly\n",
      "Hypothesis: like i wouldn't know what a nonstuttering me would even do or be you know you know the um might i have a different job possibly\n",
      "---\n",
      "Processed 80/169 files\n",
      "Reference: but um in when i'm in school like it's fine because i mean everyone should understand right i'm also like the uh i'm also the cochapter leader of the san jose uh national stuttering association support group so um yeah i'm definitely more open there\n",
      "Hypothesis: but um in what i'm in school like it's fine because you know everyone should understand right i'm also like the uh um i'm also the co chapter leader of the sanose uh national stuttering situation support group so um yeah i'm definitely more open there\n",
      "---\n",
      "Processed 90/169 files\n",
      "Reference: and then they gradually speed you up to one second a syllable half a second a syllable and then something called slow normal\n",
      "Hypothesis: and then a gradually speed you up to one second a syllable half a second a syllable and then something called slow normal\n",
      "---\n",
      "Processed 100/169 files\n",
      "Reference: um and then uh since at the time my stuttering originated uh with reading and so and then it obviously uh showed when i would speak to people too so uh but it was i couldn't tell if i was mild or i just was able to do things to kinda hide it\n",
      "Hypothesis: um and then uh since at the time my stuttering originated uh with reading and so and then it obviously uh showed what i would speak to people to so um but it was i could tell that was mild where i just was able to do things to kind of hide it\n",
      "---\n",
      "Processed 110/169 files\n",
      "Reference: and she taught me these techniques you know like um easy onset and prolongations and stuff but i don't really use it much but i feel like i do have a slower rate of speech\n",
      "Hypothesis: and she told me these techniques you know like um easy onset and prolongation stuff but i don't really use much but i feel like i do have a slow rate of speech\n",
      "---\n",
      "Processed 120/169 files\n",
      "Reference: for an in service day and i think the most positive was within a minute of my the beginning of my presentation i right away said i am a person or i'm sorry i'm a covert person who stutters and i said it out loud to twenty five colleagues in the workshop\n",
      "Hypothesis: for an in service day and i think the most positive was within a minute of my the beginning of my presentation i right away said i'm a person or i'm sorry i'm a covert person who stuttered and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five colleagues in the workshop and i said it out loud to twenty five of all right um so obviously the problem is we obviously the problem is we um so obviously the problem is we obviously the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we um so the problem is we the problem is we\n",
      "---\n",
      "Processed 130/169 files\n",
      "Reference: i didn't know why they didn't say i stuttered it was just a speech class\n",
      "Hypothesis: i didn't know why they didn't say i stuttered just in speech class\n",
      "---\n",
      "Processed 140/169 files\n",
      "Reference: i went to the covert workshop as a speech pathologist i never knew what i was doing had a name so i went and i heard all the stories\n",
      "Hypothesis: i went to the covert workshop as a speech pathologist i never knew what i was doing had a name so i went and i heard all the stories\n",
      "---\n",
      "Processed 150/169 files\n",
      "Reference: i don't know um i actually was at the mall last week surveying people\n",
      "Hypothesis: yeah um i actually was at the mall last week surveying people\n",
      "---\n",
      "Processed 160/169 files\n",
      "Reference: so um yes i did when i was younger\n",
      "Hypothesis: so um yes i did when i was younger\n",
      "---\n",
      "Fine-tuned Whisper WER on validation set: 0.2499\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on the validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_wer, val_results = evaluate_fine_tuned_whisper(val_dataset, fine_tuned_processor, fine_tuned_model, test=0)\n",
    "print(f\"Fine-tuned Whisper WER on validation set: {val_wer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Performance\n",
    "\n",
    "Let's create a summary of the model's performance across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Summary:\n",
      "Base Model Test WER: 0.3674\n",
      "Fine-tuned Model Validation WER: 0.2499\n",
      "Fine-tuned Model Test WER: 0.1897\n",
      "Improvement: 48.36%\n"
     ]
    }
   ],
   "source": [
    "# Create performance summary\n",
    "performance_summary = {\n",
    "    'base_model': {\n",
    "        'test_wer': base_wer\n",
    "    },\n",
    "    'fine_tuned_model': {\n",
    "        'validation_wer': val_wer,\n",
    "        'test_wer': fine_tuned_wer\n",
    "    },\n",
    "    'improvement': {\n",
    "        'absolute': base_wer - fine_tuned_wer,\n",
    "        'relative': (base_wer - fine_tuned_wer) / base_wer * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save performance summary\n",
    "with open(f'performance_summary-{TRAIN_NUM}.json', 'w') as f:\n",
    "    json.dump(performance_summary, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"Base Model Test WER: {base_wer:.4f}\")\n",
    "print(f\"Fine-tuned Model Validation WER: {val_wer:.4f}\")\n",
    "print(f\"Fine-tuned Model Test WER: {fine_tuned_wer:.4f}\")\n",
    "print(f\"Improvement: {(base_wer - fine_tuned_wer) / base_wer * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
